{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import utils\n",
    "\n",
    "from torchaudio import datasets, transforms\n",
    "from torchvision import models\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.LIBRISPEECH(\n",
    "                            root=\"./\",\n",
    "                            url=\"dev-clean\",\n",
    "                            folder_in_archive=\"LibriSpeech\",\n",
    "                            download=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1).features\n",
    "decoder = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "transform = transforms.MelSpectrogram(16000, n_fft=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(utils.data.Dataset):\n",
    "    def __init__(self, raw_dataset, tokenizer):\n",
    "        self.raw_dataset = raw_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.raw_dataset[idx]\n",
    "        x = data[0]\n",
    "        y = data[2]\n",
    "\n",
    "        y = self.tokenizer.encode_plus(\n",
    "                                        text=y,  # the sentence to be encoded\n",
    "                                        add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                                        max_length=512,  # maximum length of a sentence\n",
    "                                        padding=\"max_length\",  # Add [PAD]s\n",
    "                                        return_tensors='pt',  # ask the function to return PyTorch tensors\n",
    "                                        truncation=True,\n",
    "                                    )\n",
    "        return x,y[\"input_ids\"], y[\"attention_mask\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2703"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydataset = TokenizedDataset(dataset, tokenizer)\n",
    "\n",
    "len(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, transform, tokenizer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        random_input = torch.rand((1,90000))\n",
    "        random_spectrogram = self.transform(random_input)\n",
    "        random_spectrogram = random_spectrogram.repeat(1,3,1,1)\n",
    "        random_extracted_features = self.encoder(random_spectrogram)\n",
    "        self.n_filters = random_extracted_features.shape[1]\n",
    "\n",
    "        self.enc_dec_adapter = nn.Linear(in_features=self.n_filters, out_features=self.decoder.config.d_model)\n",
    "\n",
    "    def forward(self, *args):\n",
    "\n",
    "        x = args[0]\n",
    "        out = self.transform(x)\n",
    "        out = out.repeat(1, 3, 1, 1)\n",
    "        out = self.encoder(out)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        out = out.reshape(-1, int(out.shape[1] * out.shape[2] * out.shape[3]/ self.decoder.config.d_model), self.decoder.config.d_model )\n",
    "\n",
    "        if len(args) > 1:\n",
    "            y = args[1]\n",
    "            mask = args[2]\n",
    "            out = self.decoder(inputs_embeds=out, labels=y,return_dict=True, decoder_attention_mask=mask)\n",
    "        else:\n",
    "            out = self.decoder.generate(inputs_embeds=out,\n",
    "                                        max_length=1024,\n",
    "                                        min_length=0)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        out  = self.forward(x, y)\n",
    "        loss = out.loss\n",
    "        self.log(\"train_loss\")\n",
    "        return loss\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.0819, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_encoded_ids(encoded_ids_list, tokenizer):\n",
    "    phrases = []\n",
    "    for encoded_ids in encoded_ids_list:\n",
    "        decoded_ids = tokenizer.decode(encoded_ids, skip_special_tokens=True)\n",
    "        phrases.append(decoded_ids)\n",
    "    return phrases\n",
    "\n",
    "\n",
    "x = dataset[0][0]\n",
    "y = dataset[0][2]\n",
    "\n",
    "tz = tokenizer.encode_plus(\n",
    "    text=y,  # the sentence to be encoded\n",
    "    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "    max_length=512,  # maximum length of a sentence\n",
    "    padding=\"max_length\",  # Add [PAD]s\n",
    "    return_tensors='pt',  # ask the function to return PyTorch tensors\n",
    "    truncation=True,\n",
    ")\n",
    "y = tz[\"input_ids\"]\n",
    "mask = tz[\"attention_mask\"]\n",
    "print(y.shape)\n",
    "\n",
    "model = EncoderDecoder(encoder=encoder,\n",
    "                       decoder=decoder,\n",
    "                       transform=transform,\n",
    "                       tokenizer=tokenizer)\n",
    "\n",
    "model(x,y, mask).loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fed2aa9592218c4b4de25ecb09c6b292eb81f342cc059d9c96856c64676dbef4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
